{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model in PyTorch\n",
    "\n",
    "- The following file is a simple example of how to load a model in PyTorch.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Load `Bigram-model-with-architecture` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BigramLanguageModel(\n",
       "  (token_embedding_table): Embedding(65, 65)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(\n",
    "            idx\n",
    "        )  # (B,T,C) - Batch, tokens, channels (embedding size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(\n",
    "                B * T\n",
    "            )  # we could have also used the -1 argument to the view function\n",
    "            loss = F.cross_entropy(\n",
    "                logits, targets\n",
    "            )  # we reshape the logits to be a 2D tensor and the targets to be 1D for the loss function as it expects **(N, C) and (N, 1)** respectively\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        \"\"\"_summary_\n",
    "        Use only the last token in the context to generate new tokens\n",
    "        \"\"\"\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "model = torch.load(\"./bigram-model/bigram-model-with-architecture.pth\")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2. Load `Bigram-model-with-state-dict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('token_embedding_table.weight',\n",
       "              tensor([[  2.1663,  -8.9142,  -7.7765,  ...,  -9.5859,  -2.7291, -10.1730],\n",
       "                      [ -5.9427,  -4.5666,  -7.7469,  ...,  -9.2596,   0.6270,  -4.7146],\n",
       "                      [  1.9087,   1.5847, -10.8409,  ...,  -9.4011,  -9.1053,  -8.9093],\n",
       "                      ...,\n",
       "                      [ -3.1896,  -0.0140,  -8.8260,  ...,  -6.7737,  -2.1301,  -7.2833],\n",
       "                      [ -0.2109,   2.7150,  -1.2015,  ...,  -9.5673,  -8.3921,  -9.5525],\n",
       "                      [ -7.7607,  -1.5148,  -7.2700,  ...,  -6.4779,  -0.6521,  -0.7968]],\n",
       "                     device='mps:0'))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_model_with_state_dict = BigramLanguageModel(65) # initialize the model with the same architecture\n",
    "\n",
    "model_state_dict = torch.load(\"./bigram-model/bigram-model-state-dict.pth\")\n",
    "model_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BigramLanguageModel(\n",
       "  (token_embedding_table): Embedding(65, 65)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_model_with_state_dict.load_state_dict(model_state_dict)\n",
    "bigram_model_with_state_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Difference between the two methods\n",
    "\n",
    "- The first method loads the model with the architecture and the weights. This method requires `only the class` of the model to be defined in the file.\n",
    "\n",
    "- The second method loads the model with the state dictionary. This method requires `the class and the object initialized with same parameters`, and then the state dictionary is loaded into the object.\n",
    "\n",
    "---\n",
    "\n",
    "## Size difference between the two methods\n",
    "\n",
    "- The first method is larger in size as it contains the architecture and the weights.\n",
    "\n",
    "- The second method is smaller in size as it contains only the weights."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
